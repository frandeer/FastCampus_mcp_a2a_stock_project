#!/usr/bin/env python3
"""
SupervisorAgent - A2A í”„ë¡œí† ì½œ í˜¸ì¶œ ì˜ˆì œ

A2A í”„ë¡œí† ì½œì„ í†µí•´ SupervisorAgentì™€ í†µì‹ í•˜ì—¬ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
SupervisorAgentëŠ” ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í•˜ìœ„ ì—ì´ì „íŠ¸ë“¤ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.

ì‹¤í–‰ ì „ì œ ì¡°ê±´:
1. MCP ì„œë²„ë“¤ì´ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨ (Docker composeë¡œ ì‹¤í–‰ë¨)
2. SupervisorAgent A2A ì„œë²„ê°€ Docker composeë¡œ ì‹¤í–‰ë˜ì–´ ìˆì–´ì•¼ í•¨
"""

import asyncio
import json
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import structlog

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.a2a_integration.a2a_lg_client_utils_v2 import A2AClientManagerV2

logger = structlog.get_logger(__name__)

# ê°œë³„ A2A ì—ì´ì „íŠ¸ URL ì„¤ì •
AGENT_URLS = {
    "supervisor": "http://localhost:8000",
    "data_collector": "http://localhost:8001", 
    "analysis": "http://localhost:8002",
    "trading": "http://localhost:8003"
}


@dataclass
class SupervisorIntegrationTestResult:
    """SupervisorAgent í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    test_name: str
    passed: bool
    details: Dict[str, Any]
    execution_time_ms: float
    error_message: Optional[str] = None
    sub_tests: List[Dict[str, Any]] = field(default_factory=list)

    def add_sub_test(self, name: str, passed: bool, details: Any = None):
        """ì„œë¸Œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€"""
        self.sub_tests.append({
            "name": name,
            "passed": passed,
            "details": details
        })


def validate_supervisor_output(response_data: Dict[str, Any]) -> Dict[str, bool]:
    """
    SupervisorAgent ì‘ë‹µ ë°ì´í„° ê²€ì¦

    ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ê²€ì¦:
    - DATA_ONLY: ë°ì´í„°ë§Œ ìˆ˜ì§‘
    - DATA_ANALYSIS: ë°ì´í„° ìˆ˜ì§‘ + ë¶„ì„
    - FULL_WORKFLOW: ì „ì²´ ì›Œí¬í”Œë¡œìš° (ê±°ë˜ í¬í•¨)
    """
    validation_results = {
        "has_valid_structure": False,
        "has_workflow_pattern": False,
        "has_sub_agent_info": False,
        "has_coordination_result": False,
        "has_proper_format": False
    }

    try:
        # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
        if isinstance(response_data, dict):
            validation_results["has_valid_structure"] = True
            
        # ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ì •ë³´ í™•ì¸ (ì‘ë‹µì—ì„œ ì¶”ë¡  ê°€ëŠ¥í•œ íŒ¨í„´)
        content_str = str(response_data).lower()
        workflow_indicators = [
            "ë°ì´í„°", "ë¶„ì„", "ê±°ë˜", "workflow", 
            "datacollector", "analysis", "trading"
        ]
        if any(indicator in content_str for indicator in workflow_indicators):
            validation_results["has_workflow_pattern"] = True
            
        # ì„œë¸Œ ì—ì´ì „íŠ¸ í˜¸ì¶œ ì •ë³´ í™•ì¸
        sub_agent_indicators = [
            "agent", "ì—ì´ì „íŠ¸", "í˜¸ì¶œ", "ì‹¤í–‰", 
            "datacollector", "analysis", "trading"
        ]
        if any(indicator in content_str for indicator in sub_agent_indicators):
            validation_results["has_sub_agent_info"] = True
            
        # ì¡°ì • ê²°ê³¼ í™•ì¸
        coordination_indicators = [
            "ê²°ê³¼", "ì™„ë£Œ", "ì„±ê³µ", "result", "response"
        ]
        if any(indicator in content_str for indicator in coordination_indicators):
            validation_results["has_coordination_result"] = True
            
        # A2A í˜•ì‹ ì¤€ìˆ˜ í™•ì¸
        if ("content" in response_data or 
            "text_content" in response_data or 
            "data" in response_data):
            validation_results["has_proper_format"] = True
            
    except Exception as e:
        logger.error(f"ì‘ë‹µ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        
    return validation_results


async def call_individual_agent(agent_type: str, query: str) -> Dict[str, Any]:
    """ê°œë³„ A2A ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ"""
    agent_url = AGENT_URLS.get(agent_type)
    if not agent_url:
        raise ValueError(f"Unknown agent type: {agent_type}")
        
    input_data = {"messages": [{"role": "user", "content": query}]}
    
    print(f"ğŸ“ {agent_type} ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ: {agent_url}")
    print(f"   ğŸ“ ìš”ì²­: {query}")
    
    async with A2AClientManagerV2(
        base_url=agent_url,
        streaming=False,
        retry_delay=1.0,
        max_retries=3
    ) as client_manager:
        result = await client_manager.send_data(input_data)
        print(f"âœ… {agent_type} ì‘ë‹µ í¬ê¸°: {len(str(result))} ë¬¸ì")
        return result


def validate_agent_response_quality(agent_type: str, response_data: Dict[str, Any]) -> Dict[str, Any]:
    """ì—ì´ì „íŠ¸ ì‘ë‹µ í’ˆì§ˆ ìƒì„¸ ê²€ì¦"""
    validation = {
        "agent_type": agent_type,
        "response_size": len(str(response_data)),
        "has_meaningful_content": False,
        "has_proper_structure": False,
        "has_a2a_format": False,
        "content_indicators": [],
        "data_quality_score": 0.0,
        "issues": []
    }
    
    try:
        response_str = str(response_data).lower()
        
        # A2A í¬ë§· ê²€ì¦
        if any(key in response_data for key in ["content", "text_content", "data", "messages"]):
            validation["has_a2a_format"] = True
        else:
            validation["issues"].append("A2A í‘œì¤€ í¬ë§· ë¯¸ì¤€ìˆ˜")
            
        # êµ¬ì¡° ê²€ì¦
        if isinstance(response_data, dict) and len(response_data) > 0:
            validation["has_proper_structure"] = True
        else:
            validation["issues"].append("ì‘ë‹µ êµ¬ì¡° ë¶€ì ì ˆ")
            
        # ì—ì´ì „íŠ¸ë³„ íŠ¹í™” ê²€ì¦
        if agent_type == "data_collector":
            # ë°ì´í„° ìˆ˜ì§‘ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì¦
            data_indicators = ["ì£¼ê°€", "ì‹œì„¸", "ë°ì´í„°", "ì •ë³´", "price", "data", "stock", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥"]
            found_indicators = [ind for ind in data_indicators if ind in response_str]
            validation["content_indicators"] = found_indicators
            validation["has_meaningful_content"] = len(found_indicators) > 0
            validation["data_quality_score"] = min(len(found_indicators) / 3.0, 1.0)
            
        elif agent_type == "analysis":
            # ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì¦  
            analysis_indicators = ["ë¶„ì„", "í‰ê°€", "ì¶”ì²œ", "ì „ë§", "analysis", "recommendation", "rsi", "per", "ê¸°ìˆ ì ", "ê¸°ë³¸ì "]
            found_indicators = [ind for ind in analysis_indicators if ind in response_str]
            validation["content_indicators"] = found_indicators
            validation["has_meaningful_content"] = len(found_indicators) > 0
            validation["data_quality_score"] = min(len(found_indicators) / 3.0, 1.0)
            
        elif agent_type == "trading":
            # ê±°ë˜ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì¦
            trading_indicators = ["ì£¼ë¬¸", "ë§¤ìˆ˜", "ë§¤ë„", "ê±°ë˜", "trading", "order", "buy", "sell", "portfolio"]
            found_indicators = [ind for ind in trading_indicators if ind in response_str]
            validation["content_indicators"] = found_indicators  
            validation["has_meaningful_content"] = len(found_indicators) > 0
            validation["data_quality_score"] = min(len(found_indicators) / 3.0, 1.0)
            
        elif agent_type == "supervisor":
            # ì›Œí¬í”Œë¡œìš° ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì¦
            workflow_indicators = ["ì›Œí¬í”Œë¡œìš°", "íŒ¨í„´", "ë‹¨ê³„", "workflow", "pattern", "step", "coordination"]
            found_indicators = [ind for ind in workflow_indicators if ind in response_str]
            validation["content_indicators"] = found_indicators
            validation["has_meaningful_content"] = len(found_indicators) > 0  
            validation["data_quality_score"] = min(len(found_indicators) / 2.0, 1.0)
            
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        if not validation["has_meaningful_content"]:
            validation["issues"].append("ì˜ë¯¸ìˆëŠ” ì»¨í…ì¸  ë¶€ì¡±")
            
    except Exception as e:
        validation["issues"].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
    return validation


async def test_individual_agent_calls() -> SupervisorIntegrationTestResult:
    """ê°œë³„ A2A ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ í…ŒìŠ¤íŠ¸"""
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ê°œë³„ A2A ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ê° ì—ì´ì „íŠ¸ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    agent_test_cases = [
        {
            "agent": "data_collector",
            "query": "ì‚¼ì„±ì „ì í˜„ì¬ ì£¼ê°€ì™€ ê±°ë˜ëŸ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "expected_keywords": ["ì£¼ê°€", "ë°ì´í„°", "ì‹œì„¸"]
        },
        {
            "agent": "analysis", 
            "query": "ì‚¼ì„±ì „ì ê¸°ìˆ ì  ë¶„ì„ì„ í•´ì£¼ì„¸ìš” (RSI, ì´ë™í‰ê· )",
            "expected_keywords": ["ë¶„ì„", "RSI", "ê¸°ìˆ ì "]
        },
        {
            "agent": "trading",
            "query": "ì‚¼ì„±ì „ì 100ì£¼ ëª¨ì˜ ë§¤ìˆ˜ ì£¼ë¬¸ì„ í•´ì£¼ì„¸ìš”",
            "expected_keywords": ["ì£¼ë¬¸", "ë§¤ìˆ˜", "ê±°ë˜"]
        }
    ]
    
    passed_agents = 0
    total_agents = len(agent_test_cases)
    
    for test_case in agent_test_cases:
        agent_type = test_case["agent"]
        
        try:
            print(f"\nğŸ§ª {agent_type} ì—ì´ì „íŠ¸ ê°œë³„ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ
            response_data = await call_individual_agent(agent_type, test_case["query"])
            
            # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
            validation = validate_agent_response_quality(agent_type, response_data)
            
            # í…ŒìŠ¤íŠ¸ í†µê³¼ ì¡°ê±´
            agent_passed = (
                validation["has_a2a_format"] and
                validation["has_proper_structure"] and
                validation["has_meaningful_content"] and
                validation["data_quality_score"] >= 0.3
            )
            
            if agent_passed:
                passed_agents += 1
                print(f"   âœ… {agent_type} í…ŒìŠ¤íŠ¸ í†µê³¼ (í’ˆì§ˆ ì ìˆ˜: {validation['data_quality_score']:.2f})")
                print(f"      ğŸ’¡ ë°œê²¬ëœ í‚¤ì›Œë“œ: {validation['content_indicators'][:5]}")
            else:
                print(f"   âŒ {agent_type} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                print(f"      âš ï¸  ë¬¸ì œì : {validation['issues']}")
                test_result.passed = False
                
            test_result.add_sub_test(
                f"{agent_type} ì§ì ‘ í˜¸ì¶œ",
                agent_passed,
                {
                    "query": test_case["query"],
                    "validation": validation,
                    "response_preview": str(response_data)[:200] + "..."
                }
            )
            
        except Exception as e:
            print(f"   âŒ {agent_type} í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            test_result.passed = False
            test_result.add_sub_test(f"{agent_type} ì§ì ‘ í˜¸ì¶œ", False, str(e))
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_agents": passed_agents,
        "total_agents": total_agents,
        "success_rate": f"{(passed_agents/total_agents)*100:.1f}%"
    }
    
    return test_result


async def test_workflow_patterns() -> SupervisorIntegrationTestResult:
    """
    ì›Œí¬í”Œë¡œìš° íŒ¨í„´ í…ŒìŠ¤íŠ¸
    
    ë‹¤ì–‘í•œ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•´ ì ì ˆí•œ ì›Œí¬í”Œë¡œìš° íŒ¨í„´ì´ ì„ íƒë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸:
    - DATA_ONLY íŒ¨í„´: ë‹¨ìˆœ ë°ì´í„° ìš”ì²­
    - DATA_ANALYSIS íŒ¨í„´: ë¶„ì„ ìš”ì²­
    - FULL_WORKFLOW íŒ¨í„´: ê±°ë˜ í¬í•¨ ìš”ì²­
    """
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ì›Œí¬í”Œë¡œìš° íŒ¨í„´ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
    test_cases = [
        {
            "name": "DATA_ONLY íŒ¨í„´",
            "request": "ì‚¼ì„±ì „ì í˜„ì¬ ì£¼ê°€ëŠ”?",
            "expected_workflow": "data_only"
        },
        {
            "name": "DATA_ANALYSIS íŒ¨í„´", 
            "request": "ì‚¼ì„±ì „ì ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "expected_workflow": "data_analysis"
        },
        {
            "name": "FULL_WORKFLOW íŒ¨í„´",
            "request": "ì‚¼ì„±ì „ì 100ì£¼ ë§¤ìˆ˜í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
            "expected_workflow": "full_workflow"
        }
    ]
    
    passed_tests = 0
    total_tests = len(test_cases)
    
    for test_case in test_cases:
        try:
            print(f"\nğŸ§ª {test_case['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
            print(f"   ğŸ“ ìš”ì²­: {test_case['request']}")
            
            # SupervisorAgent í˜¸ì¶œ
            response_data = await call_supervisor_via_a2a(test_case['request'])
            
            # ì‘ë‹µ ê²€ì¦
            validation_results = validate_supervisor_output(response_data)
            
            # ì›Œí¬í”Œë¡œìš° íŒ¨í„´ í™•ì¸
            workflow_detected = validation_results["has_workflow_pattern"]
            agent_coordination = validation_results["has_sub_agent_info"]
            
            test_passed = workflow_detected and agent_coordination
            
            if test_passed:
                passed_tests += 1
                print(f"   âœ… {test_case['name']} ì„±ê³µ")
            else:
                print(f"   âŒ {test_case['name']} ì‹¤íŒ¨")
                test_result.passed = False
            
            test_result.add_sub_test(
                test_case['name'],
                test_passed,
                {
                    "request": test_case['request'],
                    "validation_results": validation_results,
                    "response_size": len(str(response_data))
                }
            )
            
        except Exception as e:
            print(f"   âŒ {test_case['name']} ì˜¤ë¥˜: {str(e)}")
            test_result.passed = False
            test_result.add_sub_test(test_case['name'], False, str(e))
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_tests": passed_tests,
        "total_tests": total_tests,
        "success_rate": f"{(passed_tests/total_tests)*100:.1f}%"
    }
    
    return test_result


async def test_sub_agent_orchestration() -> SupervisorIntegrationTestResult:
    """
    ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡° í…ŒìŠ¤íŠ¸
    
    SupervisorAgentê°€ í•˜ìœ„ ì—ì´ì „íŠ¸ë“¤ì„ ì˜¬ë°”ë¥´ê²Œ í˜¸ì¶œí•˜ê³ 
    ê²°ê³¼ë¥¼ ì¡°ì •í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
    """
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡° í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ë³µí•©ì ì¸ ìš”ì²­ìœ¼ë¡œ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ í˜‘ì¡° í…ŒìŠ¤íŠ¸
    complex_request = "NAVER ì£¼ì‹ì— ëŒ€í•œ ì¢…í•©ì ì¸ íˆ¬ì íŒë‹¨ì„ í•´ì£¼ì„¸ìš”"
    
    try:
        print(f"\nğŸ¤ ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡° í…ŒìŠ¤íŠ¸ ì¤‘...")
        print(f"   ğŸ“ ë³µí•© ìš”ì²­: {complex_request}")
        
        # SupervisorAgent í˜¸ì¶œ
        response_data = await call_supervisor_via_a2a(complex_request)
        
        # ì‘ë‹µ ê²€ì¦
        validation_results = validate_supervisor_output(response_data)
        
        # í˜‘ì¡° íŒ¨í„´ ê²€ì¦
        coordination_checks = {
            "workflow_pattern_detected": validation_results["has_workflow_pattern"],
            "sub_agent_info_present": validation_results["has_sub_agent_info"], 
            "coordination_result_available": validation_results["has_coordination_result"],
            "proper_a2a_format": validation_results["has_proper_format"]
        }
        
        passed_checks = sum(coordination_checks.values())
        total_checks = len(coordination_checks)
        
        if passed_checks >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ ê²€ì¦ í†µê³¼
            print(f"   âœ… ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡° ì„±ê³µ ({passed_checks}/{total_checks})")
        else:
            print(f"   âŒ ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡° ë¶€ì¡± ({passed_checks}/{total_checks})")
            test_result.passed = False
        
        test_result.add_sub_test(
            "ë³µí•© ìš”ì²­ ì²˜ë¦¬",
            passed_checks >= 3,
            {
                "coordination_checks": coordination_checks,
                "validation_results": validation_results,
                "response_length": len(str(response_data))
            }
        )
        
    except Exception as e:
        print(f"   âŒ ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
        test_result.passed = False
        test_result.error_message = str(e)
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "request": complex_request,
        "coordination_checks": coordination_checks if 'coordination_checks' in locals() else {}
    }
    
    return test_result


async def test_decision_making_logic() -> SupervisorIntegrationTestResult:
    """
    ì˜ì‚¬ê²°ì • ë¡œì§ í…ŒìŠ¤íŠ¸
    
    ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ SupervisorAgentì˜ ì˜ì‚¬ê²°ì •ì´
    ì ì ˆí•˜ê²Œ ì´ë£¨ì–´ì§€ëŠ”ì§€ í…ŒìŠ¤íŠ¸
    """
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ì˜ì‚¬ê²°ì • ë¡œì§ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ì˜ì‚¬ê²°ì •ì´ í•„ìš”í•œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤
    decision_scenarios = [
        {
            "name": "ë‹¨ìˆœ ì •ë³´ ìš”ì²­",
            "request": "ì½”ìŠ¤í”¼ ì§€ìˆ˜ ì•Œë ¤ì£¼ì„¸ìš”",
            "expected_decision": "ì •ë³´ ì œê³µ"
        },
        {
            "name": "ë¶„ì„ ìš”ì²­",
            "request": "ì‚¼ì„±ì „ì íˆ¬ì ì „ë§ì€?",
            "expected_decision": "ë¶„ì„ ìˆ˜í–‰"
        },
        {
            "name": "ëª¨í˜¸í•œ ìš”ì²­",
            "request": "ì¢‹ì€ íˆ¬ì ì¶”ì²œí•´ì£¼ì„¸ìš”",
            "expected_decision": "ëª…í™•í™” ìš”ì²­"
        }
    ]
    
    passed_decisions = 0
    total_decisions = len(decision_scenarios)
    
    for scenario in decision_scenarios:
        try:
            print(f"\nğŸ¯ {scenario['name']} ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì¤‘...")
            print(f"   ğŸ“ ìš”ì²­: {scenario['request']}")
            
            # SupervisorAgent í˜¸ì¶œ
            response_data = await call_supervisor_via_a2a(scenario['request'])
            
            # ì˜ì‚¬ê²°ì • í’ˆì§ˆ í‰ê°€
            validation_results = validate_supervisor_output(response_data)
            
            decision_quality = (
                validation_results["has_valid_structure"] and
                validation_results["has_coordination_result"] and
                validation_results["has_proper_format"]
            )
            
            if decision_quality:
                passed_decisions += 1
                print(f"   âœ… {scenario['name']} ì˜ì‚¬ê²°ì • ì„±ê³µ")
            else:
                print(f"   âŒ {scenario['name']} ì˜ì‚¬ê²°ì • ë¶€ì¡±")
                test_result.passed = False
            
            test_result.add_sub_test(
                scenario['name'],
                decision_quality,
                {
                    "request": scenario['request'],
                    "validation_results": validation_results,
                    "expected_decision": scenario['expected_decision']
                }
            )
            
        except Exception as e:
            print(f"   âŒ {scenario['name']} ì˜ì‚¬ê²°ì • ì˜¤ë¥˜: {str(e)}")
            test_result.passed = False
            test_result.add_sub_test(scenario['name'], False, str(e))
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_decisions": passed_decisions,
        "total_decisions": total_decisions,
        "decision_accuracy": f"{(passed_decisions/total_decisions)*100:.1f}%"
    }
    
    return test_result


async def test_multi_turn_conversation() -> SupervisorIntegrationTestResult:
    """ë©€í‹°í„´ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ë©€í‹°í„´ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ì—°ì†ì ì¸ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤
    conversation_scenarios = [
        {
            "name": "ì ì§„ì  íˆ¬ì ê²°ì • ê³¼ì •",
            "turns": [
                "ì‚¼ì„±ì „ì í˜„ì¬ ìƒí™©ì€ ì–´ë•Œ?",
                "ê·¸ëŸ¼ ê¸°ìˆ ì  ë¶„ì„ë„ í•´ì¤˜",
                "íˆ¬ì ì¶”ì²œì€?",
                "100ì£¼ ë§¤ìˆ˜í•˜ë©´ ì–´ë–¨ê¹Œ?"
            ],
            "expected_progression": ["ë°ì´í„°", "ë¶„ì„", "ì¶”ì²œ", "ê±°ë˜"]
        },
        {
            "name": "ë¹„êµ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤",
            "turns": [
                "ì‚¼ì„±ì „ìì™€ SKí•˜ì´ë‹‰ìŠ¤ ì£¼ê°€ ë¹„êµí•´ì¤˜",
                "ë‘˜ ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ íˆ¬ìì²˜ì•¼?",
                "ê·¸ëŸ¼ ì‚¼ì„±ì „ì 50ì£¼ ë§¤ìˆ˜ ì¶”ì²œí•´"
            ],
            "expected_progression": ["ë¹„êµë°ì´í„°", "ë¶„ì„ë¹„êµ", "íˆ¬ìê²°ì •"]
        }
    ]
    
    passed_scenarios = 0
    total_scenarios = len(conversation_scenarios)
    
    for scenario in conversation_scenarios:
        scenario_name = scenario["name"]
        turns = scenario["turns"]
        
        try:
            print(f"\nğŸ’¬ ë©€í‹°í„´ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤: {scenario_name}")
            print(f"   ğŸ“ ì´ {len(turns)}í„´ì˜ ëŒ€í™” ì§„í–‰")
            
            conversation_history = []
            turn_results = []
            scenario_passed = True
            
            for turn_num, user_input in enumerate(turns, 1):
                print(f"\n   ğŸ”„ í„´ {turn_num}: {user_input}")
                
                try:
                    # SupervisorAgent í˜¸ì¶œ (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ë¥¼ ìœ„í•´ ë™ì¼í•œ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
                    response_data = await call_supervisor_via_a2a(user_input)
                    
                    # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
                    validation = validate_agent_response_quality("supervisor", response_data)
                    
                    # í„´ë³„ ê²°ê³¼ ì €ì¥
                    turn_result = {
                        "turn": turn_num,
                        "user_input": user_input,
                        "response_quality": validation["data_quality_score"],
                        "has_meaningful_content": validation["has_meaningful_content"],
                        "response_size": validation["response_size"]
                    }
                    turn_results.append(turn_result)
                    conversation_history.append({
                        "user": user_input,
                        "assistant": str(response_data)[:100] + "..."
                    })
                    
                    # í„´ë³„ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                    turn_passed = (
                        validation["has_meaningful_content"] and
                        validation["data_quality_score"] >= 0.2 and
                        validation["response_size"] >= 10
                    )
                    
                    if turn_passed:
                        print(f"      âœ… í„´ {turn_num} ì„±ê³µ (í’ˆì§ˆ ì ìˆ˜: {validation['data_quality_score']:.2f})")
                    else:
                        print(f"      âŒ í„´ {turn_num} ì‹¤íŒ¨")
                        scenario_passed = False
                        
                except Exception as e:
                    print(f"      âŒ í„´ {turn_num} ì˜¤ë¥˜: {str(e)}")
                    scenario_passed = False
                    turn_results.append({
                        "turn": turn_num,
                        "user_input": user_input,
                        "error": str(e)
                    })
                    
                # í„´ ê°„ ê°„ê²©
                await asyncio.sleep(1)
            
            # ì „ì²´ ëŒ€í™”ì˜ ì¼ê´€ì„± ê²€ì¦
            if scenario_passed and len(turn_results) >= 2:
                # ì‘ë‹µ í’ˆì§ˆì´ ëŒ€í™”ê°€ ì§„í–‰ë ìˆ˜ë¡ í–¥ìƒë˜ëŠ”ì§€ í™•ì¸
                quality_scores = [r.get("response_quality", 0) for r in turn_results if "response_quality" in r]
                consistency_check = len(quality_scores) >= len(turns) * 0.7  # 70% ì´ìƒ ì„±ê³µì ì¸ ì‘ë‹µ
                
                if consistency_check:
                    passed_scenarios += 1
                    print(f"   âœ… {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì „ì²´ ì„±ê³µ")
                else:
                    print(f"   âš ï¸  {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì¼ê´€ì„± ë¶€ì¡±")
                    scenario_passed = False
            else:
                print(f"   âŒ {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì‹¤íŒ¨")
                
            test_result.add_sub_test(
                scenario_name,
                scenario_passed,
                {
                    "total_turns": len(turns),
                    "successful_turns": len([r for r in turn_results if r.get("response_quality", 0) > 0.2]),
                    "turn_results": turn_results,
                    "conversation_history": conversation_history
                }
            )
            
        except Exception as e:
            print(f"   âŒ {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì˜¤ë¥˜: {str(e)}")
            test_result.passed = False
            test_result.add_sub_test(scenario_name, False, str(e))
    
    if passed_scenarios < total_scenarios:
        test_result.passed = False
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_scenarios": passed_scenarios,
        "total_scenarios": total_scenarios,
        "success_rate": f"{(passed_scenarios/total_scenarios)*100:.1f}%"
    }
    
    return test_result


async def test_workflow_chain_analysis() -> SupervisorIntegrationTestResult:
    """SupervisorAgent ì›Œí¬í”Œë¡œìš° ì²´ì¸ ìƒì„¸ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ì›Œí¬í”Œë¡œìš° ì²´ì¸ ìƒì„¸ ë¶„ì„ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ì›Œí¬í”Œë¡œìš° ì²´ì¸ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    workflow_test_cases = [
        {
            "name": "DATA_ONLY ì²´ì¸ ë¶„ì„",
            "query": "ì½”ìŠ¤í”¼ ì§€ìˆ˜ í˜„ì¬ ê°’ì€?",
            "expected_pattern": "DATA_ONLY",
            "expected_agents": ["data_collector"],
            "expected_steps": ["data_collection"]
        },
        {
            "name": "DATA_ANALYSIS ì²´ì¸ ë¶„ì„", 
            "query": "ì‚¼ì„±ì „ì íˆ¬ì ì „ë§ ë¶„ì„í•´ì¤˜",
            "expected_pattern": "DATA_ANALYSIS",
            "expected_agents": ["data_collector", "analysis"],
            "expected_steps": ["data_collection", "analysis"]
        },
        {
            "name": "FULL_WORKFLOW ì²´ì¸ ë¶„ì„",
            "query": "NAVER ì£¼ì‹ 200ì£¼ ë§¤ìˆ˜ íˆ¬ì ê²°ì •í•´ì¤˜",
            "expected_pattern": "FULL_WORKFLOW", 
            "expected_agents": ["data_collector", "analysis", "trading"],
            "expected_steps": ["data_collection", "analysis", "trading"]
        }
    ]
    
    passed_workflows = 0
    total_workflows = len(workflow_test_cases)
    
    for workflow_case in workflow_test_cases:
        workflow_name = workflow_case["name"]
        
        try:
            print(f"\nğŸ”— ì›Œí¬í”Œë¡œìš° ì²´ì¸ ë¶„ì„: {workflow_name}")
            print(f"   ğŸ“ ìš”ì²­: {workflow_case['query']}")
            print(f"   ğŸ¯ ì˜ˆìƒ íŒ¨í„´: {workflow_case['expected_pattern']}")
            
            # SupervisorAgent í˜¸ì¶œ
            response_data = await call_supervisor_via_a2a(workflow_case['query'])
            
            # ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ê²€ì¦
            response_str = str(response_data).lower()
            
            # íŒ¨í„´ ê°ì§€
            pattern_detected = workflow_case['expected_pattern'].lower() in response_str
            
            # ì—ì´ì „íŠ¸ í˜¸ì¶œ í”ì  í™•ì¸
            agents_called = []
            for agent in workflow_case['expected_agents']:
                if agent in response_str or agent.replace('_', '') in response_str:
                    agents_called.append(agent)
            
            # ë‹¨ê³„ë³„ ì‹¤í–‰ í™•ì¸
            steps_found = []
            for step in workflow_case['expected_steps']:
                if step in response_str or step.replace('_', ' ') in response_str:
                    steps_found.append(step)
            
            # ì²´ì¸ ì™„ì„±ë„ ê³„ì‚°
            agent_coverage = len(agents_called) / len(workflow_case['expected_agents'])
            step_coverage = len(steps_found) / len(workflow_case['expected_steps'])
            
            # ì›Œí¬í”Œë¡œìš° ì„±ê³µ ì¡°ê±´
            workflow_passed = (
                pattern_detected and
                agent_coverage >= 0.5 and
                step_coverage >= 0.5 and
                len(str(response_data)) > 50  # ì˜ë¯¸ìˆëŠ” ì‘ë‹µ ê¸¸ì´
            )
            
            if workflow_passed:
                passed_workflows += 1
                print(f"   âœ… {workflow_name} ì„±ê³µ")
                print(f"      ğŸ“Š ì—ì´ì „íŠ¸ ì»¤ë²„ë¦¬ì§€: {agent_coverage:.1%}")
                print(f"      ğŸ“ˆ ë‹¨ê³„ ì»¤ë²„ë¦¬ì§€: {step_coverage:.1%}")
                print(f"      ğŸ’¡ í˜¸ì¶œëœ ì—ì´ì „íŠ¸: {agents_called}")
            else:
                print(f"   âŒ {workflow_name} ì‹¤íŒ¨")
                print(f"      âš ï¸  íŒ¨í„´ ê°ì§€: {'âœ…' if pattern_detected else 'âŒ'}")
                print(f"      âš ï¸  ì—ì´ì „íŠ¸ ì»¤ë²„ë¦¬ì§€: {agent_coverage:.1%}")
                print(f"      âš ï¸  ë‹¨ê³„ ì»¤ë²„ë¦¬ì§€: {step_coverage:.1%}")
                test_result.passed = False
            
            test_result.add_sub_test(
                workflow_name,
                workflow_passed,
                {
                    "query": workflow_case['query'],
                    "expected_pattern": workflow_case['expected_pattern'],
                    "pattern_detected": pattern_detected,
                    "expected_agents": workflow_case['expected_agents'],
                    "agents_called": agents_called,
                    "agent_coverage": agent_coverage,
                    "expected_steps": workflow_case['expected_steps'],
                    "steps_found": steps_found,
                    "step_coverage": step_coverage,
                    "response_size": len(str(response_data))
                }
            )
            
        except Exception as e:
            print(f"   âŒ {workflow_name} ì˜¤ë¥˜: {str(e)}")
            test_result.passed = False
            test_result.add_sub_test(workflow_name, False, str(e))
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_workflows": passed_workflows,
        "total_workflows": total_workflows,
        "success_rate": f"{(passed_workflows/total_workflows)*100:.1f}%"
    }
    
    return test_result


async def test_error_handling_and_resilience() -> SupervisorIntegrationTestResult:
    """ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µì›ë ¥ í…ŒìŠ¤íŠ¸"""
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µì›ë ¥ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    error_test_cases = [
        {
            "name": "ì˜ëª»ëœ ìš”ì²­ ì²˜ë¦¬",
            "query": "",  # ë¹ˆ ìš”ì²­
            "expected_behavior": "graceful_error_handling"
        },
        {
            "name": "ëª¨í˜¸í•œ ìš”ì²­ ì²˜ë¦¬",
            "query": "sdkfjslkdfjslkdf ì•Œë ¤ì¤˜",  # ë¬´ì˜ë¯¸í•œ ìš”ì²­
            "expected_behavior": "clarification_request"
        },
        {
            "name": "ë³µì¡í•œ ìš”ì²­ ì²˜ë¦¬",
            "query": "ëª¨ë“  ì£¼ì‹ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ ì™„ë²½í•œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë§Œë“¤ì–´ì¤˜ ì§€ê¸ˆ ë‹¹ì¥",  # ê³¼ë„í•œ ìš”ì²­
            "expected_behavior": "reasonable_response"
        }
    ]
    
    passed_error_tests = 0
    total_error_tests = len(error_test_cases)
    
    for error_case in error_test_cases:
        test_name = error_case["name"]
        
        try:
            print(f"\nğŸ›¡ï¸ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸: {test_name}")
            print(f"   ğŸ“ ìš”ì²­: '{error_case['query']}'")
            
            # SupervisorAgent í˜¸ì¶œ (ì—ëŸ¬ ìƒí™©)
            response_data = await call_supervisor_via_a2a(error_case['query'])
            
            # ì—ëŸ¬ ì²˜ë¦¬ í’ˆì§ˆ ê²€ì¦
            response_str = str(response_data).lower()
            error_indicators = ["ì˜¤ë¥˜", "error", "ì˜ëª»", "ëª…í™•", "ë‹¤ì‹œ", "êµ¬ì²´ì "]
            
            graceful_handling = any(indicator in response_str for indicator in error_indicators)
            has_response = len(response_str) > 10
            not_crashed = isinstance(response_data, dict)
            
            # ì—ëŸ¬ ì²˜ë¦¬ ì„±ê³µ ì¡°ê±´
            error_test_passed = graceful_handling and has_response and not_crashed
            
            if error_test_passed:
                passed_error_tests += 1
                print(f"   âœ… {test_name} ì„±ê³µ: ìš°ì•„í•œ ì—ëŸ¬ ì²˜ë¦¬")
                print(f"      ğŸ’¡ ë°œê²¬ëœ ì—ëŸ¬ ì²˜ë¦¬ ì§€í‘œ: {[ind for ind in error_indicators if ind in response_str][:3]}")
            else:
                print(f"   âŒ {test_name} ì‹¤íŒ¨")
                print(f"      âš ï¸  ìš°ì•„í•œ ì²˜ë¦¬: {'âœ…' if graceful_handling else 'âŒ'}")
                print(f"      âš ï¸  ì‘ë‹µ ì¡´ì¬: {'âœ…' if has_response else 'âŒ'}")
                print(f"      âš ï¸  ì‹œìŠ¤í…œ ì•ˆì •ì„±: {'âœ…' if not_crashed else 'âŒ'}")
                test_result.passed = False
            
            test_result.add_sub_test(
                test_name,
                error_test_passed,
                {
                    "query": error_case['query'],
                    "expected_behavior": error_case['expected_behavior'],
                    "graceful_handling": graceful_handling,
                    "response_size": len(response_str),
                    "system_stable": not_crashed,
                    "error_indicators_found": [ind for ind in error_indicators if ind in response_str]
                }
            )
            
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒë„ ì–´ëŠ ì •ë„ëŠ” ì˜ˆìƒë˜ëŠ” ìƒí™©
            print(f"   âš ï¸  {test_name} ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            # ì™„ì „í•œ ì‹œìŠ¤í…œ í¬ë˜ì‹œê°€ ì•„ë‹ˆë¼ë©´ ë¶€ë¶„ ì ìˆ˜
            if "timeout" not in str(e).lower():
                test_result.add_sub_test(test_name, True, f"ì˜ˆìƒëœ ì˜ˆì™¸: {str(e)}")
            else:
                test_result.passed = False
                test_result.add_sub_test(test_name, False, str(e))
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_error_tests": passed_error_tests,
        "total_error_tests": total_error_tests,
        "success_rate": f"{(passed_error_tests/total_error_tests)*100:.1f}%"
    }
    
    return test_result


async def test_performance_and_stability() -> SupervisorIntegrationTestResult:
    """ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    performance_test_cases = [
        {
            "name": "ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸",
            "query": "ì‚¼ì„±ì „ì í˜„ì¬ ì‹œì„¸ ì•Œë ¤ì£¼ì„¸ìš”",
            "max_response_time_ms": 10000  # 10ì´ˆ ì œí•œ
        },
        {
            "name": "ë™ì‹œ ìš”ì²­ ì²˜ë¦¬",
            "query": "ì½”ìŠ¤í”¼ ì§€ìˆ˜ëŠ”?",
            "concurrent_requests": 3
        },
        {
            "name": "ì¥ì‹œê°„ ì‘ë‹µ í…ŒìŠ¤íŠ¸",
            "query": "ì‚¼ì„±ì „ì, LGì „ì, SKí•˜ì´ë‹‰ìŠ¤ ë¹„êµ ë¶„ì„í•´ì¤˜",
            "max_response_time_ms": 30000  # 30ì´ˆ ì œí•œ
        }
    ]
    
    passed_performance_tests = 0
    total_performance_tests = len(performance_test_cases)
    
    for perf_case in performance_test_cases:
        test_name = perf_case["name"]
        
        try:
            print(f"\nâš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: {test_name}")
            
            if perf_case["name"] == "ë™ì‹œ ìš”ì²­ ì²˜ë¦¬":
                # ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸
                print(f"   ğŸ“ ë™ì‹œ ìš”ì²­ {perf_case['concurrent_requests']}ê°œ ì „ì†¡")
                
                start_concurrent = time.time()
                tasks = []
                for i in range(perf_case['concurrent_requests']):
                    task = call_supervisor_via_a2a(f"{perf_case['query']} (ìš”ì²­ #{i+1})")
                    tasks.append(task)
                
                # ëª¨ë“  ë™ì‹œ ìš”ì²­ ì‹¤í–‰
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                concurrent_time = (time.time() - start_concurrent) * 1000
                
                # ê²°ê³¼ ê²€ì¦
                successful_responses = sum(1 for r in responses if not isinstance(r, Exception))
                concurrency_success = successful_responses >= perf_case['concurrent_requests'] * 0.7  # 70% ì„±ê³µ
                
                if concurrency_success:
                    passed_performance_tests += 1
                    print(f"   âœ… {test_name} ì„±ê³µ: {successful_responses}/{perf_case['concurrent_requests']} ì‘ë‹µ ({concurrent_time:.0f}ms)")
                else:
                    print(f"   âŒ {test_name} ì‹¤íŒ¨: {successful_responses}/{perf_case['concurrent_requests']} ì‘ë‹µ")
                    test_result.passed = False
                
                test_result.add_sub_test(
                    test_name,
                    concurrency_success,
                    {
                        "concurrent_requests": perf_case['concurrent_requests'],
                        "successful_responses": successful_responses,
                        "total_time_ms": concurrent_time,
                        "avg_response_time_ms": concurrent_time / perf_case['concurrent_requests']
                    }
                )
                
            else:
                # ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
                print(f"   ğŸ“ ìš”ì²­: {perf_case['query']}")
                print(f"   â±ï¸  ì œí•œ ì‹œê°„: {perf_case['max_response_time_ms']}ms")
                
                request_start = time.time()
                response_data = await call_supervisor_via_a2a(perf_case['query'])
                response_time = (time.time() - request_start) * 1000
                
                # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
                performance_ok = (
                    response_time <= perf_case['max_response_time_ms'] and
                    isinstance(response_data, dict) and
                    len(str(response_data)) > 10
                )
                
                if performance_ok:
                    passed_performance_tests += 1
                    print(f"   âœ… {test_name} ì„±ê³µ: {response_time:.0f}ms (ê¸°ì¤€: {perf_case['max_response_time_ms']}ms)")
                else:
                    print(f"   âŒ {test_name} ì‹¤íŒ¨: {response_time:.0f}ms (ê¸°ì¤€ ì´ˆê³¼)")
                    test_result.passed = False
                
                test_result.add_sub_test(
                    test_name,
                    performance_ok,
                    {
                        "query": perf_case['query'],
                        "response_time_ms": response_time,
                        "max_allowed_ms": perf_case['max_response_time_ms'],
                        "within_limit": response_time <= perf_case['max_response_time_ms'],
                        "response_size": len(str(response_data))
                    }
                )
                
        except Exception as e:
            print(f"   âŒ {test_name} ì˜¤ë¥˜: {str(e)}")
            test_result.passed = False
            test_result.add_sub_test(test_name, False, str(e))
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_performance_tests": passed_performance_tests,
        "total_performance_tests": total_performance_tests,
        "success_rate": f"{(passed_performance_tests/total_performance_tests)*100:.1f}%"
    }
    
    return test_result


async def test_real_world_scenarios() -> SupervisorIntegrationTestResult:
    """ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ í…ŒìŠ¤íŠ¸"""
    import time
    start_time = time.time()
    
    test_result = SupervisorIntegrationTestResult(
        test_name="ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ í…ŒìŠ¤íŠ¸",
        passed=True,
        details={},
        execution_time_ms=0
    )
    
    # ì‹¤ì œ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ ì¼€ì´ìŠ¤
    real_world_scenarios = [
        {
            "name": "ì´ˆë³´ íˆ¬ìì ì •ë³´ ìš”ì²­",
            "query": "ì£¼ì‹ íˆ¬ì ì²˜ìŒ í•´ë³´ëŠ”ë° ì‚¼ì„±ì „ì ì–´ë•Œìš”?",
            "expected_workflow": "DATA_ANALYSIS",
            "expected_content": ["ë°ì´í„°", "ë¶„ì„", "ì¶”ì²œ"]
        },
        {
            "name": "ê²½í—˜ì ê¸°ìˆ ì  ë¶„ì„ ìš”ì²­",
            "query": "ì‚¼ì„±ì „ì RSI, MACD ê°™ì€ ê¸°ìˆ ì  ì§€í‘œ ë¶„ì„ ë¶€íƒí•´",
            "expected_workflow": "DATA_ANALYSIS", 
            "expected_content": ["ê¸°ìˆ ì ", "ì§€í‘œ", "ë¶„ì„"]
        },
        {
            "name": "ì‹¤ì œ ë§¤ë§¤ ì˜ë¢°",
            "query": "ì‚¼ì„±ì „ì 100ì£¼ ë§¤ìˆ˜í•˜ê³  ì‹¶ì€ë° ì§€ê¸ˆì´ ì¢‹ì€ íƒ€ì´ë°ì¸ê°€ìš”?",
            "expected_workflow": "FULL_WORKFLOW",
            "expected_content": ["ë¶„ì„", "ë§¤ìˆ˜", "íƒ€ì´ë°"]
        },
        {
            "name": "í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì–¸ ìš”ì²­",
            "query": "IT ì„¹í„°ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±í•˜ê³  ì‹¶ì€ë° ì–´ë–¤ ì¢…ëª©ë“¤ì´ ì¢‹ì„ê¹Œìš”?",
            "expected_workflow": "DATA_ANALYSIS",
            "expected_content": ["í¬íŠ¸í´ë¦¬ì˜¤", "ì„¹í„°", "ì¢…ëª©"]
        },
        {
            "name": "ê¸‰ë½ ìƒí™© ëŒ€ì‘ ë¬¸ì˜",
            "query": "ë³´ìœ  ì£¼ì‹ì´ ê¸‰ë½í•˜ê³  ìˆëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
            "expected_workflow": "DATA_ANALYSIS",
            "expected_content": ["ê¸‰ë½", "ëŒ€ì‘", "ë¶„ì„"]
        }
    ]
    
    passed_scenarios = 0
    total_scenarios = len(real_world_scenarios)
    
    for scenario in real_world_scenarios:
        scenario_name = scenario["name"]
        
        try:
            print(f"\nğŸŒ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸: {scenario_name}")
            print(f"   ğŸ“ ì‚¬ìš©ì ë¬¸ì˜: {scenario['query']}")
            print(f"   ğŸ¯ ì˜ˆìƒ ì›Œí¬í”Œë¡œìš°: {scenario['expected_workflow']}")
            
            # SupervisorAgent í˜¸ì¶œ
            response_data = await call_supervisor_via_a2a(scenario['query'])
            
            # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
            response_str = str(response_data).lower()
            
            # ì˜ˆìƒ ì»¨í…ì¸  í¬í•¨ ì—¬ë¶€ í™•ì¸
            content_matches = []
            for expected_content in scenario['expected_content']:
                if expected_content in response_str:
                    content_matches.append(expected_content)
            
            content_coverage = len(content_matches) / len(scenario['expected_content'])
            
            # ì‘ë‹µì˜ ì‹¤ìš©ì„± ê²€ì¦
            response_length = len(response_str)
            has_meaningful_response = response_length > 50
            has_structured_format = isinstance(response_data, dict)
            
            # ì‹œë‚˜ë¦¬ì˜¤ ì„±ê³µ ê¸°ì¤€
            scenario_success = (
                content_coverage >= 0.5 and  # 50% ì´ìƒì˜ ì˜ˆìƒ ì»¨í…ì¸  í¬í•¨
                has_meaningful_response and
                has_structured_format
            )
            
            if scenario_success:
                passed_scenarios += 1
                print(f"   âœ… {scenario_name} ì„±ê³µ")
                print(f"      ğŸ“Š ì»¨í…ì¸  ì»¤ë²„ë¦¬ì§€: {content_coverage:.1%}")
                print(f"      ğŸ’¡ ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {content_matches}")
            else:
                print(f"   âŒ {scenario_name} ì‹¤íŒ¨")
                print(f"      âš ï¸  ì»¨í…ì¸  ì»¤ë²„ë¦¬ì§€: {content_coverage:.1%}")
                print(f"      âš ï¸  ì‘ë‹µ ê¸¸ì´: {response_length} ë¬¸ì")
                test_result.passed = False
            
            test_result.add_sub_test(
                scenario_name,
                scenario_success,
                {
                    "query": scenario['query'],
                    "expected_workflow": scenario['expected_workflow'],
                    "expected_content": scenario['expected_content'],
                    "content_matches": content_matches,
                    "content_coverage": content_coverage,
                    "response_length": response_length,
                    "has_meaningful_response": has_meaningful_response
                }
            )
            
        except Exception as e:
            print(f"   âŒ {scenario_name} ì˜¤ë¥˜: {str(e)}")
            test_result.passed = False
            test_result.add_sub_test(scenario_name, False, str(e))
    
    test_result.execution_time_ms = (time.time() - start_time) * 1000
    test_result.details = {
        "passed_scenarios": passed_scenarios,
        "total_scenarios": total_scenarios,
        "success_rate": f"{(passed_scenarios/total_scenarios)*100:.1f}%"
    }
    
    return test_result


async def run_supervisor_integration_tests() -> List[SupervisorIntegrationTestResult]:
    """ëª¨ë“  SupervisorAgent í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    print("\n" + "="*80)
    print("ğŸ§ª SupervisorAgent A2A í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    
    test_results = []
    
    try:
        # 1. ê°œë³„ A2A ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸ“ í…ŒìŠ¤íŠ¸ 1: ê°œë³„ A2A ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ':-^60}")
        individual_result = await test_individual_agent_calls()
        test_results.append(individual_result)
        
        # 2. ì›Œí¬í”Œë¡œìš° íŒ¨í„´ í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸ¯ í…ŒìŠ¤íŠ¸ 2: ì›Œí¬í”Œë¡œìš° íŒ¨í„´':-^60}")
        workflow_result = await test_workflow_patterns()
        test_results.append(workflow_result)
        
        # 3. ì›Œí¬í”Œë¡œìš° ì²´ì¸ ìƒì„¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸ”— í…ŒìŠ¤íŠ¸ 3: ì›Œí¬í”Œë¡œìš° ì²´ì¸ ìƒì„¸ ë¶„ì„':-^60}")
        chain_result = await test_workflow_chain_analysis()
        test_results.append(chain_result)
        
        # 4. ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡° í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸ¤ í…ŒìŠ¤íŠ¸ 4: ì„œë¸Œ ì—ì´ì „íŠ¸ í˜‘ì¡°':-^60}")
        orchestration_result = await test_sub_agent_orchestration()
        test_results.append(orchestration_result)
        
        # 5. ë©€í‹°í„´ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸ’¬ í…ŒìŠ¤íŠ¸ 5: ë©€í‹°í„´ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤':-^60}")
        multiturn_result = await test_multi_turn_conversation()
        test_results.append(multiturn_result)
        
        # 6. ì˜ì‚¬ê²°ì • ë¡œì§ í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸ¯ í…ŒìŠ¤íŠ¸ 6: ì˜ì‚¬ê²°ì • ë¡œì§':-^60}")
        decision_result = await test_decision_making_logic()
        test_results.append(decision_result)
        
        # 7. ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µì›ë ¥ í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸ›¡ï¸ í…ŒìŠ¤íŠ¸ 7: ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µì›ë ¥':-^60}")
        error_result = await test_error_handling_and_resilience()
        test_results.append(error_result)
        
        # 8. ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ í…ŒìŠ¤íŠ¸
        print(f"\n{'ğŸŒ í…ŒìŠ¤íŠ¸ 8: ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤':-^60}")
        realworld_result = await test_real_world_scenarios()
        test_results.append(realworld_result)
        
        # 9. ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸
        print(f"\n{'âš¡ í…ŒìŠ¤íŠ¸ 9: ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦':-^60}")
        performance_result = await test_performance_and_stability()
        test_results.append(performance_result)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\n{'ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½':-^60}")
        passed_tests = sum(1 for result in test_results if result.passed)
        total_tests = len(test_results)
        
        for result in test_results:
            status = "âœ… PASS" if result.passed else "âŒ FAIL"
            print(f"   {status} {result.test_name} ({result.execution_time_ms:.0f}ms)")
            
            if result.sub_tests:
                for sub_test in result.sub_tests:
                    sub_status = "âœ…" if sub_test["passed"] else "âŒ"
                    print(f"      {sub_status} {sub_test['name']}")
        
        print(f"\nğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {passed_tests}/{total_tests} ({(passed_tests/total_tests)*100:.1f}%)")
        
        # ìƒì„¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = Path("logs/examples/a2a")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        detailed_results = {
            "timestamp": timestamp,
            "test_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "success_rate": f"{(passed_tests/total_tests)*100:.1f}%"
            },
            "test_results": [
                {
                    "test_name": result.test_name,
                    "passed": result.passed,
                    "execution_time_ms": result.execution_time_ms,
                    "details": result.details,
                    "error_message": result.error_message,
                    "sub_tests": result.sub_tests
                }
                for result in test_results
            ]
        }
        
        output_file = output_dir / f"supervisor_integration_test_results_{timestamp}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"\nâŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    return test_results


async def call_supervisor_via_a2a(user_request: str) -> Dict[str, Any]:
    """A2A í”„ë¡œí† ì½œì„ í†µí•´ SupervisorAgent í˜¸ì¶œ"""

    # SupervisorAgent A2A ì„œë²„ URL(ë„ì»¤)
    supervisor_url = "http://localhost:8000"

    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ - SupervisorAgentëŠ” ì‚¬ìš©ì ì§ˆë¬¸ë§Œ í•„ìš”
    input_data = {
        "messages": [{"content": user_request, "role": "user"}]
    }

    print("\nğŸ“¤ SupervisorAgent ìš”ì²­ ì „ì†¡:")
    print(f"   ğŸ“ ì‚¬ìš©ì ìš”ì²­: '{user_request}'")

    async with A2AClientManagerV2(
        base_url=supervisor_url,
        streaming=False,
        retry_delay=2.0
    ) as client_manager:
        try:
            response_data = await client_manager.send_data(input_data)
            logger.info(f"response_data: {response_data}")
            print("\nğŸ“¥ SupervisorAgent ì‘ë‹µ ìˆ˜ì‹ :")
            print(f"   {json.dumps(response_data, ensure_ascii=False, indent=2)}")

            # JSON íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = Path("logs/examples/a2a")
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / f"a2a_supervisor_result_{timestamp}.json"

            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(response_data, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            return response_data

        except Exception as e:
            print(f"âŒ A2A í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise


async def test_custom_supervisor_a2a():
    """CustomSupervisorAgent A2A í†µì‹  í…ŒìŠ¤íŠ¸"""

    print(f"\n{'='*70}")
    print("CustomSupervisorAgent A2A í…ŒìŠ¤íŠ¸")
    print('='*70)

    try:
        # A2Aë¥¼ í†µí•œ SupervisorAgent í˜¸ì¶œ
        print("ğŸš€ A2A í”„ë¡œí† ì½œì„ í†µí•´ CustomSupervisorAgent ì‹¤í–‰ ì¤‘...")
        await call_supervisor_via_a2a("ì‚¼ì„±ì „ì í˜„ì¬ ì£¼ê°€ë¥¼ ì¡°íšŒí•˜ê³ , ë§¤ìˆ˜ê°€ ì ì ˆí•œì§€ ë¶„ì„í•œ ë’¤ì— í‰ê°€í•˜ê³  ì‹¤ì œ ë§¤ë„ë¥¼ ì‹œì¥ê°€ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ A2A í”„ë¡œí† ì½œì„ í†µí•œ SupervisorAgent ì›Œí¬í”Œë¡œìš° ê²€ì¦")
    print("ğŸ¤ í•˜ìœ„ ì—ì´ì „íŠ¸ë“¤ê³¼ì˜ í˜‘ì—… íŒ¨í„´ í…ŒìŠ¤íŠ¸")

    # ê¸°ë³¸ SupervisorAgent í…ŒìŠ¤íŠ¸
    await test_custom_supervisor_a2a()

    # # í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # integration_results = await run_supervisor_integration_tests()
    
    # # ì „ì²´ ê²°ê³¼ ìš”ì•½
    # print(f"\n{'ğŸ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼':-^70}")
    # total_integration_tests = len(integration_results)
    # passed_integration_tests = sum(1 for result in integration_results if result.passed)
    
    # if total_integration_tests > 0:
    #     success_rate = (passed_integration_tests / total_integration_tests) * 100
    #     print(f"   ğŸ“Š í†µí•© í…ŒìŠ¤íŠ¸: {passed_integration_tests}/{total_integration_tests} ({success_rate:.1f}%)")
        
    #     if success_rate >= 80:
    #         print("   ğŸ‰ SupervisorAgent A2A í†µí•©ì´ ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤!")
    #     else:
    #         print("   âš ï¸  SupervisorAgent A2A í†µí•©ì— ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # print(f"   ğŸ“ ìƒì„¸ ê²°ê³¼ëŠ” logs/examples/a2a/ ë””ë ‰í† ë¦¬ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())