"""
Step 0: LLM ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
OpenAI GPT-4o-miniì™€ ì—°ê²°í•´ì„œ ê¸°ë³¸ í˜¸ì¶œì´ ë˜ëŠ”ì§€ í™•ì¸

ì´ ì˜ˆì œëŠ”:
1. OpenAI APIë¥¼ ì‚¬ìš©í•´ì„œ GPT-4o-mini í˜¸ì¶œ
2. ê°„ë‹¨í•œ ì§ˆë¬¸-ì‘ë‹µ í…ŒìŠ¤íŠ¸
3. ë‚˜ì¤‘ì— ë„êµ¬ í˜¸ì¶œê³¼ ì—°ê²°í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ êµ¬ì¶•
"""

import asyncio
import json
import os
import sys
from pathlib import Path
from typing import Dict, Any
from openai import AsyncOpenAI

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
sys.path.append(str(Path(__file__).parent.parent))
from common_utils import get_api_key, get_setting, print_environment_status

class SimpleLLMClient:
    def __init__(self, api_key: str = None):
        """
        OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            api_key: OpenAI API í‚¤. Noneì´ë©´ .env íŒŒì¼ì´ë‚˜ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´
        """
        if api_key is None:
            api_key = get_api_key('OPENAI_API_KEY', required=True)
            if not api_key:
                raise ValueError(
                    "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                    ".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”."
                )
        
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = "gpt-4o-mini"  # GPT-4o-mini ì‚¬ìš©
    
    async def simple_chat(self, message: str) -> str:
        """ê°„ë‹¨í•œ ì±„íŒ… í˜¸ì¶œ"""
        try:
            print(f"ğŸ¤– AIì—ê²Œ ì§ˆë¬¸: {message}")
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": message}
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content.strip()
            print(f"ğŸ’¬ AI ì‘ë‹µ: {answer}")
            return answer
            
        except Exception as e:
            error_msg = f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    async def chat_with_system_prompt(self, system_prompt: str, user_message: str) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ ì±„íŒ…"""
        try:
            print(f"ğŸ“‹ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}")
            print(f"ğŸ¤– ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content.strip()
            print(f"ğŸ’¬ AI ì‘ë‹µ: {answer}")
            return answer
            
        except Exception as e:
            error_msg = f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg

async def test_llm_basic():
    """LLM ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ LLM ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # í™˜ê²½ ì„¤ì • ìƒíƒœ ì¶œë ¥
    print_environment_status()
    
    # API í‚¤ í™•ì¸ (ì´ë¯¸ ìœ„ì—ì„œ ì¶œë ¥ë¨)
    api_key = get_api_key('OPENAI_API_KEY', required=False)  # ì—ëŸ¬ ë©”ì‹œì§€ ì¤‘ë³µ ë°©ì§€
    if not api_key:
        print("\nğŸ’¡ .env íŒŒì¼ ì„¤ì • ë°©ë²•:")
        print("   1. learning_examples/.env.exampleì„ learning_examples/.envë¡œ ë³µì‚¬")
        print("   2. OPENAI_API_KEY=your-key-here ì„¤ì •")
        return
    
    # LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    llm = SimpleLLMClient()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {
            "name": "ê°„ë‹¨í•œ ì¸ì‚¬ í…ŒìŠ¤íŠ¸",
            "type": "simple",
            "message": "ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?"
        },
        {
            "name": "ìˆ˜í•™ ë¬¸ì œ í…ŒìŠ¤íŠ¸", 
            "type": "simple",
            "message": "15 + 23ì€ ì–¼ë§ˆì¸ê°€ìš”? ê³„ì‚° ê³¼ì •ë„ ë³´ì—¬ì£¼ì„¸ìš”."
        },
        {
            "name": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸",
            "type": "system",
            "system_prompt": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤. í•­ìƒ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê³  ê²©ë ¤ì˜ ë§ì„ í•´ì£¼ì„¸ìš”.",
            "user_message": "7 Ã— 8ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”."
        }
    ]
    
    # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ {i}: {test_case['name']}")
        print("-" * 30)
        
        if test_case['type'] == 'simple':
            result = await llm.simple_chat(test_case['message'])
        elif test_case['type'] == 'system':
            result = await llm.chat_with_system_prompt(
                test_case['system_prompt'], 
                test_case['user_message']
            )
        
        if not result.startswith("LLM í˜¸ì¶œ ì‹¤íŒ¨"):
            print("âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
            
        print()  # ë¹ˆ ì¤„
        
        # API í˜¸ì¶œ ì§€ì—° (.envì—ì„œ ì„¤ì • ê°€ëŠ¥)
        delay = get_setting('LEARNING_API_DELAY', 1, int)
        await asyncio.sleep(delay)
    
    print("=" * 50)
    print("ğŸ‰ LLM ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    print("ğŸ’¡ ì‚¬ìš©ë²•:")
    print("1. OpenAI API í‚¤ ì„¤ì •: export OPENAI_API_KEY='your-key'")
    print("2. ì‹¤í–‰: python simple_llm_test.py")
    print()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_llm_basic())