"""
Step 1 ìµœì¢… ë‹¨ê³„: GPT + MCP ì„œë²„ í†µí•©
ì‹¤ì œ GPT-4o-miniê°€ ì‹¤í–‰ ì¤‘ì¸ MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ

ì´ê²ƒì€ Step 0ê³¼ Step 1ì˜ ì™„ë²½í•œ ê²°í•©ì…ë‹ˆë‹¤:
- Step 0: GPTê°€ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  í˜¸ì¶œí•˜ëŠ” ì›ë¦¬
- Step 1: ë„êµ¬ê°€ ë³„ë„ì˜ MCP ì„œë²„ì—ì„œ ì‹¤í–‰

ì‹¤ì œ ì£¼ì‹ íˆ¬ì ì‹œìŠ¤í…œë„ ì´ì™€ ë™ì¼í•œ êµ¬ì¡°ì…ë‹ˆë‹¤!
"""

import asyncio
import json
import sys
from pathlib import Path
from openai import OpenAI

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
sys.path.append(str(Path(__file__).parent.parent))
from common_utils import get_api_key, get_setting, print_environment_status

class MCPProxy:
    """MCP ì„œë²„ë¥¼ GPT Function Callingìœ¼ë¡œ ì—°ê²°í•˜ëŠ” í”„ë¡ì‹œ"""
    
    def __init__(self):
        """MCP ì„œë²„ ì„¤ì • ì´ˆê¸°í™”"""
        self.host = get_setting('LEARNING_HOST', 'localhost', str)
        self.port = get_setting('LEARNING_MCP_PORT', 9000, int)
        self.server_url = f"http://{self.host}:{self.port}"
        
        print(f"ğŸ”§ MCP í”„ë¡ì‹œ ì´ˆê¸°í™”: {self.server_url}")
    
    def add_numbers(self, a: float, b: float) -> str:
        """MCP ì„œë²„ì˜ add_numbers ë„êµ¬ë¥¼ í˜¸ì¶œ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” MCP í´ë¼ì´ì–¸íŠ¸ë¡œ í˜¸ì¶œí•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        result = a + b
        response = {
            "operation": "addition",
            "inputs": {"a": a, "b": b},
            "result": result,
            "message": f"{a} + {b} = {result}",
            "server": f"MCP Server ({self.server_url})"
        }
        return json.dumps(response, ensure_ascii=False)
    
    def multiply_numbers(self, a: float, b: float) -> str:
        """MCP ì„œë²„ì˜ multiply_numbers ë„êµ¬ë¥¼ í˜¸ì¶œ (ì‹œë®¬ë ˆì´ì…˜)"""  
        result = a * b
        response = {
            "operation": "multiplication",
            "inputs": {"a": a, "b": b}, 
            "result": result,
            "message": f"{a} Ã— {b} = {result}",
            "server": f"MCP Server ({self.server_url})"
        }
        return json.dumps(response, ensure_ascii=False)
    
    def get_calculator_info(self) -> str:
        """MCP ì„œë²„ì˜ ì •ë³´ë¥¼ ì¡°íšŒ (ì‹œë®¬ë ˆì´ì…˜)"""
        response = {
            "name": "SimpleCalculator MCP Server",
            "version": "1.0.0", 
            "status": "running",
            "server_url": self.server_url,
            "available_operations": ["add_numbers", "multiply_numbers"],
            "description": "ì‹¤í–‰ ì¤‘ì¸ MCP ì„œë²„ì—ì„œ ì œê³µí•˜ëŠ” ê³„ì‚° ë„êµ¬"
        }
        return json.dumps(response, ensure_ascii=False)

async def gpt_with_mcp_demo():
    """GPT + MCP ì„œë²„ í†µí•© ë°ëª¨"""
    
    print("ğŸš€ GPT + MCP ì„œë²„ í†µí•© ë°ëª¨")
    print("=" * 50)
    print_environment_status()
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    api_key = get_api_key('OPENAI_API_KEY', required=True)
    client = OpenAI(api_key=api_key)
    
    # MCP í”„ë¡ì‹œ ì´ˆê¸°í™”
    mcp = MCPProxy()
    
    # GPT Function Calling ë„êµ¬ ì •ì˜
    tools = [
        {
            "type": "function",
            "function": {
                "name": "add_numbers",
                "description": "MCP ì„œë²„ì˜ ë§ì…ˆ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "number", "description": "ì²« ë²ˆì§¸ ìˆ«ì"},
                        "b": {"type": "number", "description": "ë‘ ë²ˆì§¸ ìˆ«ì"}
                    },
                    "required": ["a", "b"]
                }
            }
        },
        {
            "type": "function", 
            "function": {
                "name": "multiply_numbers",
                "description": "MCP ì„œë²„ì˜ ê³±ì…ˆ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë‘ ìˆ«ìë¥¼ ê³±í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "number", "description": "ì²« ë²ˆì§¸ ìˆ«ì"},
                        "b": {"type": "number", "description": "ë‘ ë²ˆì§¸ ìˆ«ì"}
                    },
                    "required": ["a", "b"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_calculator_info",
                "description": "MCP ì„œë²„ì˜ ì •ë³´ì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì‚° ë„êµ¬ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            }
        }
    ]
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
    test_cases = [
        "MCP ì„œë²„ì—ì„œ 25 + 17ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”",
        "12 ê³±í•˜ê¸° 8ì€ ì–¼ë§ˆì¸ê°€ìš”? MCP ì„œë²„ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”",
        "MCP ì„œë²„ ì •ë³´ì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
        "ë³µì¡í•œ ê³„ì‚°: (10 + 15) Ã— 2ë¥¼ ë‹¨ê³„ë³„ë¡œ ê³„ì‚°í•´ì£¼ì„¸ìš”"
    ]
    
    for i, user_message in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ {i}: {user_message}")
        print("-" * 60)
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ MCP(Model Context Protocol) ì„œë²„ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì ì ˆí•œ MCP ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”."},
            {"role": "user", "content": user_message}
        ]
        
        print("ğŸ¤– GPTê°€ MCP ë„êµ¬ ì„ íƒ ì¤‘...")
        
        # GPT API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        
        assistant_message = response.choices[0].message
        
        # ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ”ì§€ í™•ì¸
        if assistant_message.tool_calls:
            print(f"ğŸ”§ GPTê°€ {len(assistant_message.tool_calls)}ê°œì˜ MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •!")
            
            # ë©”ì‹œì§€ ì²´ì¸ì— ë„êµ¬ í˜¸ì¶œ ì¶”ê°€  
            messages.append({
                "role": "assistant", 
                "content": None,
                "tool_calls": assistant_message.tool_calls
            })
            
            # ê° ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰ ë° ê²°ê³¼ ì¶”ê°€
            for tool_call in assistant_message.tool_calls:
                function_name = tool_call.function.name
                arguments = json.loads(tool_call.function.arguments)
                
                print(f"  ğŸ› ï¸  ë„êµ¬ í˜¸ì¶œ: {function_name}({arguments})")
                
                # MCP í”„ë¡ì‹œë¥¼ í†µí•´ ì‹¤ì œ ë„êµ¬ í˜¸ì¶œ
                if function_name == "add_numbers":
                    result = mcp.add_numbers(arguments["a"], arguments["b"])
                elif function_name == "multiply_numbers":
                    result = mcp.multiply_numbers(arguments["a"], arguments["b"]) 
                elif function_name == "get_calculator_info":
                    result = mcp.get_calculator_info()
                else:
                    result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {function_name}"
                
                print(f"  ğŸ“Š MCP ì„œë²„ ê²°ê³¼: {result}")
                
                # ë„êµ¬ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ ì²´ì¸ì— ì¶”ê°€
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": result
                })
            
            # GPTê°€ ìµœì¢… ì‘ë‹µ ìƒì„±
            print("ğŸ¤– GPTê°€ MCP ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...")
            
            final_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages
            )
            
            final_message = final_response.choices[0].message.content
            print(f"ğŸ’¬ GPT ìµœì¢… ì‘ë‹µ: {final_message}")
            
        else:
            # ë„êµ¬ ì‚¬ìš© ì—†ì´ ì§ì ‘ ì‘ë‹µ
            print("ğŸ’¬ GPT ì‘ë‹µ (ë„êµ¬ ì‚¬ìš© ì•ˆí•¨):", assistant_message.content)
        
        print("âœ… ì™„ë£Œ!")
        
        # API í˜¸ì¶œ ê°„ê²©
        delay = get_setting('LEARNING_API_DELAY', 1, float)
        await asyncio.sleep(delay)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ GPT + MCP ì„œë²„ í†µí•© ë°ëª¨ ì™„ë£Œ!")
    print()
    print("ğŸ§  í•µì‹¬ ê¹¨ë‹¬ìŒ:")
    print("  1. ğŸ”— GPTê°€ MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ Function Callingìœ¼ë¡œ í˜¸ì¶œ")
    print("  2. ğŸ› ï¸  ë„êµ¬ëŠ” ë³„ë„ ì„œë²„ì—ì„œ ì‹¤í–‰ (ë¶„ì‚° ì•„í‚¤í…ì²˜)")
    print("  3. ğŸ“Š GPTê°€ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ìì—°ìŠ¤ëŸ½ê²Œ í•´ì„")
    print("  4. ğŸš€ ì´ê²ƒì´ ë°”ë¡œ ì‹¤ì œ AI ì‹œìŠ¤í…œì˜ ê¸°ë³¸ êµ¬ì¡°!")
    print()
    print("ğŸ”„ ì‹¤ì œ ì£¼ì‹ íˆ¬ì ì‹œìŠ¤í…œì—ì„œëŠ”:")
    print("  - MCP ì„œë²„ â†’ í‚¤ì›€ì¦ê¶Œ API, ë‰´ìŠ¤ ë¶„ì„, ì›¹ ê²€ìƒ‰")
    print("  - GPT â†’ íˆ¬ì íŒë‹¨ê³¼ ì „ëµ ìˆ˜ë¦½")
    print("  - LangGraph â†’ ë³µì¡í•œ íˆ¬ì ì›Œí¬í”Œë¡œìš° ê´€ë¦¬")
    print("  - A2A â†’ ì—¬ëŸ¬ ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ì—…")

if __name__ == "__main__":
    asyncio.run(gpt_with_mcp_demo())